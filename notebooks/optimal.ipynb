{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714f409d",
   "metadata": {},
   "source": [
    "# Combining Learnings\n",
    "\n",
    "Through testing with strucutral-based link prediction and community-based link prediction, we've seen that an optimal combination of both of these techniques should yield a more optimal model. Here, we will implement this:\n",
    "\n",
    "For this model testing we will only work with 1000 artists from era 5 with successful API calls. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b681f6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5673764, 3), (1045947, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Imports\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "artists = pd.read_parquet('../data/final_data_processed/discogs_artists.parquet')\n",
    "edges = pd.read_parquet('../data/final_data_processed/discogs_edges.parquet') # Note: These will be our nodes\n",
    "\n",
    "edges.shape, artists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbb3f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Era 5\n",
      "2338167 620672\n",
      "  discogs_artist_id\n",
      "0            115466\n",
      "1            833554\n",
      "2              1768\n",
      "3             65718\n",
      "4            882018\n"
     ]
    }
   ],
   "source": [
    "# Note: First we will sort the edges chronologically\n",
    "edges_sorted = edges.sort_values(by='release_year')\n",
    "edges_sorted[:100]\n",
    "\n",
    "era5_edges = edges_sorted[(edges_sorted['release_year'] >= 2000) & (edges_sorted['release_year'] <= 2025)]\n",
    "\n",
    "era5_artists = pd.DataFrame({'discogs_artist_id': pd.unique(pd.concat([era5_edges.source_id, era5_edges.target_id]))})\n",
    "print('Era 5')\n",
    "print(len(era5_edges), len(era5_artists))\n",
    "print(era5_artists.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7968972",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data_raw/discogs_20251101_artists.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print(df.head())\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# read the artists xml file so we can use the artist IDs from the parquet file to get artist names\u001b[39;00m\n\u001b[32m     11\u001b[39m rows = []\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[43mET\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterparse\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data_raw/discogs_20251101_artists.xml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem.tag == \u001b[33m\"\u001b[39m\u001b[33martist\u001b[39m\u001b[33m\"\u001b[39m:   \u001b[38;5;66;03m# repeating entry tag for Discogs artists\u001b[39;00m\n\u001b[32m     14\u001b[39m         row = {child.tag: child.text \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/musicnet/lib/python3.11/xml/etree/ElementTree.py:1243\u001b[39m, in \u001b[36miterparse\u001b[39m\u001b[34m(source, events, parser)\u001b[39m\n\u001b[32m   1240\u001b[39m pullparser = XMLPullParser(events=events, _parser=parser)\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     source = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m     close_source = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data_raw/discogs_20251101_artists.xml'"
     ]
    }
   ],
   "source": [
    "# sample for 1000 nodes that have API Call Hits \n",
    "\n",
    "# ========== Get Artist Names ==========\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "df_artistsID = pd.read_parquet(\"../data/final_data_processed/discogs_artists.parquet\")\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "# read the artists xml file so we can use the artist IDs from the parquet file to get artist names\n",
    "rows = []\n",
    "for event, elem in ET.iterparse(\"../data_raw/discogs_20251101_artists.xml\", events=(\"end\",)):\n",
    "    if elem.tag == \"artist\":   # repeating entry tag for Discogs artists\n",
    "        row = {child.tag: child.text for child in elem}\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        # free memory\n",
    "        elem.clear()\n",
    "        parent = elem.getparent() if hasattr(elem, \"getparent\") else None\n",
    "        if parent is not None:\n",
    "            while parent.getprevious() is not None:\n",
    "                del parent[0]\n",
    "\n",
    "df_artistsNames = pd.DataFrame(rows)\n",
    "print(df_artistsNames.head(-5))\n",
    "\n",
    "\n",
    "\n",
    "# ========== SPOTIFY API FUNCTIONS ==========\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "load_dotenv()  # loads .env into environment\n",
    "\n",
    "client_id = os.getenv(\"SPOTIPY_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "auth_manager = SpotifyClientCredentials(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "# testing to see if the API call works\n",
    "artist = sp.artist(\"1uNFoZAHBGtllmzznpCI3s\")  # Justin Bieber ID\n",
    "print(artist[\"name\"])\n",
    "# method \n",
    "def get_artist(artist, market):\n",
    "    results = sp.search(q=f\"artist:{artist}\", type=\"artist\", limit=1, market=market)\n",
    "    items = results.get(\"artists\", {}).get(\"items\", [])\n",
    "\n",
    "    \n",
    "    return items[0] if items else None\n",
    "\n",
    "\n",
    "def get_artist_data(artist_name, market):\n",
    "    artist = get_artist(artist_name, market)\n",
    "    if artist:\n",
    "        return artist.get(\"genres\", [])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ========== GET 1000 ARTISTS WITH SUCCESSFUL SPOTIFY API CALLS ==========\n",
    "artist_genres = {}\n",
    "\n",
    "for index, row in era5_artists.iterrows():\n",
    "    artist_id = row['discogs_artist_id']\n",
    "    artist_name_row = df_artistsNames[df_artistsNames['id'] == str(artist_id)]\n",
    "    if not artist_name_row.empty:\n",
    "        artist_name = artist_name_row.iloc[0]['name']\n",
    "        genres = get_artist_data(artist_name, market=\"US\")\n",
    "        if genres is not None:\n",
    "            artist_genres[artist_id] = {\n",
    "                'name': artist_name,\n",
    "                'genres': genres\n",
    "            }\n",
    "    if len(artist_genres) >= 1000:\n",
    "        break\n",
    "\n",
    "print(f\"Successfully retrieved genres for {len(artist_genres)} artists.\")\n",
    "print(list(artist_genres.items())[:5])  # Print first 5 entries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered edges count: 45596\n",
      "        source_id target_id  release_year\n",
      "723768     115466   1165499          2000\n",
      "3519660    833554    454293          2000\n",
      "3519661    882018    859036          2000\n",
      "4283352    967916   1674540          2000\n",
      "1304982    396036   2816584          2000\n",
      "Graph has 1000 nodes and 4208 edges.\n"
     ]
    }
   ],
   "source": [
    "# Get all edges where both source and target artists are in artist_genres\n",
    "filtered_edges = era5_edges[\n",
    "    era5_edges['source_id'].isin(artist_genres.keys()) &\n",
    "    era5_edges['target_id'].isin(artist_genres.keys())\n",
    "]\n",
    "\n",
    "print(f\"Filtered edges count: {len(filtered_edges)}\")\n",
    "print(filtered_edges.head())\n",
    "\n",
    "# Create the graph with the filtered edges and nodes\n",
    "# add nodes\n",
    "G = nx.Graph()\n",
    "\n",
    "for artist_id in artist_genres.keys():\n",
    "    G.add_node(artist_id, name=artist_genres[artist_id]['name'], genres=artist_genres[artist_id]['genres']) \n",
    "\n",
    "# add edges\n",
    "for _, row in filtered_edges.iterrows():\n",
    "    G.add_edge(row['source_id'], row['target_id'], release_year=row['release_year'])\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220aff60",
   "metadata": {},
   "source": [
    "### Prediction Design\n",
    "\n",
    "We want to create a hybrid, weighted model combining structural and genre-based features:\n",
    "\n",
    "**Model:** Score(u, v) = w1×CN + w2×JC + w3×GenreJaccard + w4×GenreCount + w5×SameGenre + w6×CommOverlap\n",
    "\n",
    "Where:\n",
    "- **CN** = Common Neighbors (structural)\n",
    "- **JC** = Jaccard Coefficient (structural)\n",
    "- **GenreJaccard** = Genre similarity (Jaccard on genre sets)\n",
    "- **GenreCount** = Raw count of shared genres\n",
    "- **SameGenre** = Binary flag (1 if any shared genre)\n",
    "- **CommOverlap** = Community co-membership (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c21b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_id target_id  common_neighbors  jaccard_coefficient  genre_overlap  \\\n",
      "0    115466    833554          0.157025             0.226190       0.333333   \n",
      "1    115466      1768          0.000000             0.000000       0.000000   \n",
      "2    115466    882018          0.107438             0.166667       0.000000   \n",
      "3    115466    967916          0.008264             0.015152       0.000000   \n",
      "4    115466    265660          0.008264             0.015152       0.000000   \n",
      "\n",
      "   community_overlap  edge_exists  \n",
      "0                  0            0  \n",
      "1                  0            0  \n",
      "2                  0            0  \n",
      "3                  0            0  \n",
      "4                  0            0  \n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# --- 1. Get all node pairs ---\n",
    "all_nodes = list(G.nodes())\n",
    "all_pairs = list(itertools.combinations(all_nodes, 2))  # undirected, no self-loops\n",
    "\n",
    "# --- 2. Precompute clustering/community info if needed ---\n",
    "partition = community_louvain.best_partition(G)\n",
    "clustering = nx.clustering(G)  # optional if you want local clustering later\n",
    "\n",
    "rows = []\n",
    "for u, v in all_pairs:\n",
    "    # Common Neighbors\n",
    "    cn = len(list(nx.common_neighbors(G, u, v)))\n",
    "\n",
    "    # Jaccard\n",
    "    jaccard = list(nx.jaccard_coefficient(G, [(u, v)]))[0][2]\n",
    "\n",
    "    # Genre Features (3 types)\n",
    "    genres_u = set(artist_genres.get(u, {}).get('genres', []))\n",
    "    genres_v = set(artist_genres.get(v, {}).get('genres', []))\n",
    "    \n",
    "    intersection = genres_u & genres_v\n",
    "    union = genres_u | genres_v\n",
    "    \n",
    "    # Genre Jaccard (normalized overlap)\n",
    "    if union:\n",
    "        genre_jaccard = len(intersection) / len(union)\n",
    "    else:\n",
    "        genre_jaccard = 0\n",
    "    \n",
    "    # Genre Overlap (raw count)\n",
    "    genre_overlap_count = len(intersection)\n",
    "    \n",
    "    # Same Genre Flag (binary)\n",
    "    same_genre = 1 if len(intersection) > 0 else 0\n",
    "\n",
    "    # Community Overlap\n",
    "    cu = partition.get(u, -1)\n",
    "    cv = partition.get(v, -1)\n",
    "    co = 1 if cu != -1 and cu == cv else 0\n",
    "\n",
    "    # Edge label (1 if edge exists, 0 otherwise)\n",
    "    label = 1 if G.has_edge(u, v) else 0\n",
    "\n",
    "    rows.append((u, v, cn, jaccard, genre_jaccard, genre_overlap_count, same_genre, co, label))\n",
    "\n",
    "# --- 3. Make DataFrame ---\n",
    "link_pred_df = pd.DataFrame(rows, columns=[\n",
    "    'source_id', 'target_id', 'common_neighbors', \n",
    "    'jaccard_coefficient', 'genre_jaccard', 'genre_overlap_count', 'same_genre',\n",
    "    'community_overlap', 'edge_exists'\n",
    "])\n",
    "\n",
    "# Normalize common neighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "link_pred_df['common_neighbors'] = scaler.fit_transform(link_pred_df[['common_neighbors']])\n",
    "\n",
    "print(link_pred_df.head())\n",
    "print(f\"\\nFeatures: {[col for col in link_pred_df.columns if col not in ['source_id', 'target_id', 'edge_exists']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4febb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.9933253253253254\n",
      "Best Parameters: CN_WEIGHT=0.6, JACCARD_WEIGHT=0.0, GENRE_WEIGHT=0.0, COMMUNITY_WEIGHT=0.4\n"
     ]
    }
   ],
   "source": [
    "# ========== BACKTESTING THE FULL LINK PREDICTION MODEL ==========\n",
    "def predict(common_neighbors, jaccard_coefficient, genre_jaccard, genre_overlap_count, same_genre, community_overlap, \n",
    "            CN_WEIGHT, JACCARD_WEIGHT, GENRE_JACCARD_WEIGHT, GENRE_COUNT_WEIGHT, SAME_GENRE_WEIGHT, COMMUNITY_WEIGHT):\n",
    "    # Weighted sum model with all features\n",
    "    score = (CN_WEIGHT * common_neighbors +\n",
    "             JACCARD_WEIGHT * jaccard_coefficient +\n",
    "             GENRE_JACCARD_WEIGHT * genre_jaccard +\n",
    "             GENRE_COUNT_WEIGHT * genre_overlap_count +\n",
    "             SAME_GENRE_WEIGHT * same_genre +\n",
    "             COMMUNITY_WEIGHT * community_overlap)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Normalize genre_overlap_count for fair comparison\n",
    "scaler_genre = MinMaxScaler()\n",
    "link_pred_df['genre_overlap_count_norm'] = scaler_genre.fit_transform(link_pred_df[['genre_overlap_count']])\n",
    "\n",
    "# Backtest each parameter between 0 and 1 with step size 0.2\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "print(\"Starting hyperparameter search...\")\n",
    "param_range = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "for CN_WEIGHT in param_range:\n",
    "    for JACCARD_WEIGHT in param_range:\n",
    "        for GENRE_JACCARD_WEIGHT in param_range:\n",
    "            for GENRE_COUNT_WEIGHT in param_range:\n",
    "                for SAME_GENRE_WEIGHT in param_range:\n",
    "                    for COMMUNITY_WEIGHT in param_range:\n",
    "                        # Normalize weights\n",
    "                        total_weight = (CN_WEIGHT + JACCARD_WEIGHT + GENRE_JACCARD_WEIGHT + \n",
    "                                      GENRE_COUNT_WEIGHT + SAME_GENRE_WEIGHT + COMMUNITY_WEIGHT)\n",
    "                        if total_weight == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        CN_W = CN_WEIGHT / total_weight\n",
    "                        JACCARD_W = JACCARD_WEIGHT / total_weight\n",
    "                        GENRE_JACCARD_W = GENRE_JACCARD_WEIGHT / total_weight\n",
    "                        GENRE_COUNT_W = GENRE_COUNT_WEIGHT / total_weight\n",
    "                        SAME_GENRE_W = SAME_GENRE_WEIGHT / total_weight\n",
    "                        COMMUNITY_W = COMMUNITY_WEIGHT / total_weight\n",
    "                        \n",
    "                        # Make predictions\n",
    "                        link_pred_df['predicted_score'] = link_pred_df.apply(\n",
    "                            lambda row: predict(\n",
    "                                row['common_neighbors'], \n",
    "                                row['jaccard_coefficient'], \n",
    "                                row['genre_jaccard'],\n",
    "                                row['genre_overlap_count_norm'],\n",
    "                                row['same_genre'],\n",
    "                                row['community_overlap'],\n",
    "                                CN_W, JACCARD_W, GENRE_JACCARD_W, GENRE_COUNT_W, SAME_GENRE_W, COMMUNITY_W\n",
    "                            ), axis=1)\n",
    "                        \n",
    "                        # Classify based on threshold 0.5\n",
    "                        link_pred_df['predicted_label'] = (link_pred_df['predicted_score'] >= 0.5).astype(int)\n",
    "                        \n",
    "                        # Calculate accuracy\n",
    "                        accuracy = (link_pred_df['predicted_label'] == link_pred_df['edge_exists']).mean()\n",
    "                        \n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_accuracy = accuracy\n",
    "                            best_params = (CN_W, JACCARD_W, GENRE_JACCARD_W, GENRE_COUNT_W, SAME_GENRE_W, COMMUNITY_W)\n",
    "                    \n",
    "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Best Parameters:\")\n",
    "print(f\"  CN_WEIGHT: {best_params[0]:.3f}\")\n",
    "print(f\"  JACCARD_WEIGHT: {best_params[1]:.3f}\")\n",
    "print(f\"  GENRE_JACCARD_WEIGHT: {best_params[2]:.3f}\")\n",
    "print(f\"  GENRE_COUNT_WEIGHT: {best_params[3]:.3f}\")\n",
    "print(f\"  SAME_GENRE_WEIGHT: {best_params[4]:.3f}\")\n",
    "print(f\"  COMMUNITY_WEIGHT: {best_params[5]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9854267184587799\n",
      "Precision-Recall AUC: 0.456117109078475\n",
      "Hits@100: 0.023727137913989126\n"
     ]
    }
   ],
   "source": [
    "# ========== AUC-ROC EVALUATION ==========\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(link_pred_df['edge_exists'], link_pred_df['predicted_score'])\n",
    "print(f\"AUC-ROC: {auc}\")\n",
    "\n",
    "# ========== Precision-Recall EVALUATION ==========\n",
    "from sklearn.metrics import precision_recall_curve, auc as auc_metric\n",
    "precision, recall, _ = precision_recall_curve(link_pred_df['edge_exists'], link_pred_df['predicted_score'])\n",
    "pr_auc = auc_metric(recall, precision)\n",
    "print(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "\n",
    "# ========== Hits@K EVALUATION ==========   \n",
    "def hits_at_k(df, k):\n",
    "    # Sort by predicted score descending\n",
    "    df_sorted = df.sort_values(by='predicted_score', ascending=False)\n",
    "    \n",
    "    # Get top K predictions\n",
    "    top_k = df_sorted.head(k)\n",
    "    \n",
    "    # Calculate Hits@K\n",
    "    hits = top_k['edge_exists'].sum()\n",
    "    total_positives = df['edge_exists'].sum()\n",
    "    \n",
    "    return hits / total_positives if total_positives > 0 else 0 \n",
    "hits_k = 100\n",
    "hits_at_100 = hits_at_k(link_pred_df, hits_k)\n",
    "print(f\"Hits@{hits_k}: {hits_at_100}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
