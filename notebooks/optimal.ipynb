{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714f409d",
   "metadata": {},
   "source": [
    "# Combining Learnings\n",
    "\n",
    "Through testing with strucutral-based link prediction and community-based link prediction, we've seen that an optimal combination of both of these techniques should yield a more optimal model. Here, we will implement this:\n",
    "\n",
    "For this model testing we will only work with 1000 artists from era 5 with successful API calls. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b681f6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5673764, 3), (1045947, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Imports\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "artists = pd.read_parquet('../data/final_data_processed/discogs_artists.parquet')\n",
    "edges = pd.read_parquet('../data/final_data_processed/discogs_edges.parquet') # Note: These will be our nodes\n",
    "\n",
    "edges.shape, artists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbb3f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Era 5\n",
      "2338167 620672\n",
      "  discogs_artist_id\n",
      "0            115466\n",
      "1            833554\n",
      "2              1768\n",
      "3             65718\n",
      "4            882018\n"
     ]
    }
   ],
   "source": [
    "# Note: First we will sort the edges chronologically\n",
    "edges_sorted = edges.sort_values(by='release_year')\n",
    "edges_sorted[:100]\n",
    "\n",
    "era5_edges = edges_sorted[(edges_sorted['release_year'] >= 2000) & (edges_sorted['release_year'] <= 2025)]\n",
    "\n",
    "era5_artists = pd.DataFrame({'discogs_artist_id': pd.unique(pd.concat([era5_edges.source_id, era5_edges.target_id]))})\n",
    "print('Era 5')\n",
    "print(len(era5_edges), len(era5_artists))\n",
    "print(era5_artists.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7968972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id                    name         realname  \\\n",
      "0               1           The Persuader  Jesper Dahlbäck   \n",
      "1               2  Mr. James Barth & A.D.              NaN   \n",
      "2               3               Josh Wink   Josh Winkelman   \n",
      "3               4           Johannes Heil    Johannes Heil   \n",
      "4               5              Heiko Laux       Heiko Laux   \n",
      "...           ...                     ...              ...   \n",
      "9798342  16856335             Izumi Kohki              NaN   \n",
      "9798343  16856338                  Kate08        Kate Webb   \n",
      "9798344  16856341   The Evil B-Side Twins              NaN   \n",
      "9798345  16856347             Carol Lundy              NaN   \n",
      "9798346  16856350            문선 (Moonsun)              NaN   \n",
      "\n",
      "                                                   profile  \\\n",
      "0        Electronic artist working out of Stockholm, ac...   \n",
      "1                                                      NaN   \n",
      "2        Electronic music DJ, label owner, producer, an...   \n",
      "3        Electronic music producer, musician and live p...   \n",
      "4        German DJ and producer based in Berlin. He is ...   \n",
      "...                                                    ...   \n",
      "9798342                                                NaN   \n",
      "9798343                                                NaN   \n",
      "9798344                                                NaN   \n",
      "9798345                                                NaN   \n",
      "9798346                                                NaN   \n",
      "\n",
      "                data_quality  urls  namevariations  aliases  members  groups  \n",
      "0                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "1                    Correct   NaN             NaN      NaN      NaN     NaN  \n",
      "2                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "3                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "4                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "...                      ...   ...             ...      ...      ...     ...  \n",
      "9798342  Needs Major Changes   NaN             NaN      NaN      NaN     NaN  \n",
      "9798343           Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "9798344  Needs Major Changes   NaN             NaN      NaN      NaN     NaN  \n",
      "9798345  Needs Major Changes   NaN             NaN      NaN      NaN     NaN  \n",
      "9798346           Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "\n",
      "[9798347 rows x 10 columns]\n",
      "Justin Bieber\n",
      "Successfully retrieved genres for 1000 artists.\n",
      "[('115466', {'name': 'Nikolai Rimsky-Korsakov', 'genres': ['classical']}), ('833554', {'name': 'Christa Ludwig', 'genres': ['opera', 'requiem', 'classical']}), ('1768', {'name': 'Bedrock', 'genres': ['trance', 'progressive house']}), ('882018', {'name': 'Jon Vickers', 'genres': ['opera']}), ('967916', {'name': 'Halfdan Kjerulf', 'genres': []})]\n"
     ]
    }
   ],
   "source": [
    "# sample for 1000 nodes that have API Call Hits \n",
    "\n",
    "# ========== Get Artist Names ==========\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "df_artistsID = pd.read_parquet(\"../data/final_data_processed/discogs_artists.parquet\")\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "# read the artists xml file so we can use the artist IDs from the parquet file to get artist names\n",
    "rows = []\n",
    "for event, elem in ET.iterparse(\"../data_raw/discogs_20251101_artists.xml\", events=(\"end\",)):\n",
    "    if elem.tag == \"artist\":   # repeating entry tag for Discogs artists\n",
    "        row = {child.tag: child.text for child in elem}\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        # free memory\n",
    "        elem.clear()\n",
    "        parent = elem.getparent() if hasattr(elem, \"getparent\") else None\n",
    "        if parent is not None:\n",
    "            while parent.getprevious() is not None:\n",
    "                del parent[0]\n",
    "\n",
    "df_artistsNames = pd.DataFrame(rows)\n",
    "print(df_artistsNames.head(-5))\n",
    "\n",
    "\n",
    "\n",
    "# ========== SPOTIFY API FUNCTIONS ==========\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "load_dotenv()  # loads .env into environment\n",
    "\n",
    "client_id = os.getenv(\"SPOTIPY_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "auth_manager = SpotifyClientCredentials(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "# testing to see if the API call works\n",
    "artist = sp.artist(\"1uNFoZAHBGtllmzznpCI3s\")  # Justin Bieber ID\n",
    "print(artist[\"name\"])\n",
    "# method \n",
    "def get_artist(artist, market):\n",
    "    results = sp.search(q=f\"artist:{artist}\", type=\"artist\", limit=1, market=market)\n",
    "    items = results.get(\"artists\", {}).get(\"items\", [])\n",
    "\n",
    "    \n",
    "    return items[0] if items else None\n",
    "\n",
    "\n",
    "def get_artist_data(artist_name, market):\n",
    "    artist = get_artist(artist_name, market)\n",
    "    if artist:\n",
    "        return artist.get(\"genres\", [])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ========== GET 1000 ARTISTS WITH SUCCESSFUL SPOTIFY API CALLS ==========\n",
    "artist_genres = {}\n",
    "\n",
    "for index, row in era5_artists.iterrows():\n",
    "    artist_id = row['discogs_artist_id']\n",
    "    artist_name_row = df_artistsNames[df_artistsNames['id'] == str(artist_id)]\n",
    "    if not artist_name_row.empty:\n",
    "        artist_name = artist_name_row.iloc[0]['name']\n",
    "        genres = get_artist_data(artist_name, market=\"US\")\n",
    "        if genres is not None:\n",
    "            artist_genres[artist_id] = {\n",
    "                'name': artist_name,\n",
    "                'genres': genres\n",
    "            }\n",
    "    if len(artist_genres) >= 1000:\n",
    "        break\n",
    "\n",
    "print(f\"Successfully retrieved genres for {len(artist_genres)} artists.\")\n",
    "print(list(artist_genres.items())[:5])  # Print first 5 entries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ce9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered edges count: 45596\n",
      "        source_id target_id  release_year\n",
      "723768     115466   1165499          2000\n",
      "3519660    833554    454293          2000\n",
      "3519661    882018    859036          2000\n",
      "4283352    967916   1674540          2000\n",
      "1304982    396036   2816584          2000\n",
      "Graph has 1000 nodes and 4208 edges.\n"
     ]
    }
   ],
   "source": [
    "# Get all edges where both source and target artists are in artist_genres\n",
    "filtered_edges = era5_edges[\n",
    "    era5_edges['source_id'].isin(artist_genres.keys()) &\n",
    "    era5_edges['target_id'].isin(artist_genres.keys())\n",
    "]\n",
    "\n",
    "print(f\"Filtered edges count: {len(filtered_edges)}\")\n",
    "print(filtered_edges.head())\n",
    "\n",
    "# Create the graph with the filtered edges and nodes\n",
    "# add nodes\n",
    "G = nx.Graph()\n",
    "\n",
    "for artist_id in artist_genres.keys():\n",
    "    G.add_node(artist_id, name=artist_genres[artist_id]['name'], genres=artist_genres[artist_id]['genres']) \n",
    "\n",
    "# add edges\n",
    "for _, row in filtered_edges.iterrows():\n",
    "    G.add_edge(row['source_id'], row['target_id'], release_year=row['release_year'])\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220aff60",
   "metadata": {},
   "source": [
    "### Prediction Design\n",
    "\n",
    "We want to create a hybrid, weighted model combining structural and genre-based features:\n",
    "\n",
    "**Model:** Score(u, v) = w1×CN + w2×JC + w3×GenreJaccard + w4×GenreCount + w5×SameGenre + w6×CommOverlap\n",
    "\n",
    "Where:\n",
    "- **CN** = Common Neighbors (structural)\n",
    "- **JC** = Jaccard Coefficient (structural)\n",
    "- **GenreJaccard** = Genre similarity (Jaccard on genre sets)\n",
    "- **GenreCount** = Raw count of shared genres\n",
    "- **SameGenre** = Binary flag (1 if any shared genre)\n",
    "- **CommOverlap** = Community co-membership (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39c21b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_id target_id  common_neighbors  jaccard_coefficient  genre_jaccard  \\\n",
      "0    115466    833554          0.157025             0.226190       0.333333   \n",
      "1    115466      1768          0.000000             0.000000       0.000000   \n",
      "2    115466    882018          0.107438             0.166667       0.000000   \n",
      "3    115466    967916          0.008264             0.015152       0.000000   \n",
      "4    115466    265660          0.008264             0.015152       0.000000   \n",
      "\n",
      "   genre_overlap_count  same_genre  community_overlap  edge_exists  \n",
      "0                    1           1                  1            0  \n",
      "1                    0           0                  0            0  \n",
      "2                    0           0                  1            0  \n",
      "3                    0           0                  0            0  \n",
      "4                    0           0                  0            0  \n",
      "(499500, 9)\n",
      "\n",
      "Features: ['common_neighbors', 'jaccard_coefficient', 'genre_jaccard', 'genre_overlap_count', 'same_genre', 'community_overlap']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "\n",
    "# --- 1. Get all node pairs ---\n",
    "all_nodes = list(G.nodes())\n",
    "all_pairs = list(itertools.combinations(all_nodes, 2))  # undirected, no self-loops\n",
    "\n",
    "# --- 2. Precompute clustering/community info if needed ---\n",
    "partition = community_louvain.best_partition(G)\n",
    "clustering = nx.clustering(G)  # optional if you want local clustering later\n",
    "\n",
    "rows = []\n",
    "for u, v in all_pairs:\n",
    "    # Common Neighbors\n",
    "    cn = len(list(nx.common_neighbors(G, u, v)))\n",
    "\n",
    "    # Jaccard\n",
    "    jaccard = list(nx.jaccard_coefficient(G, [(u, v)]))[0][2]\n",
    "\n",
    "    # Genre Features (3 types)\n",
    "    genres_u = set(artist_genres.get(u, {}).get('genres', []))\n",
    "    genres_v = set(artist_genres.get(v, {}).get('genres', []))\n",
    "    \n",
    "    intersection = genres_u & genres_v\n",
    "    union = genres_u | genres_v\n",
    "    \n",
    "    # Genre Jaccard (normalized overlap)\n",
    "    if union:\n",
    "        genre_jaccard = len(intersection) / len(union)\n",
    "    else:\n",
    "        genre_jaccard = 0\n",
    "    \n",
    "    # Genre Overlap (raw count)\n",
    "    genre_overlap_count = len(intersection)\n",
    "    \n",
    "    # Same Genre Flag (binary)\n",
    "    same_genre = 1 if len(intersection) > 0 else 0\n",
    "\n",
    "    # Community Overlap\n",
    "    cu = partition.get(u, -1)\n",
    "    cv = partition.get(v, -1)\n",
    "    co = 1 if cu != -1 and cu == cv else 0\n",
    "\n",
    "    # Edge label (1 if edge exists, 0 otherwise)\n",
    "    label = 1 if G.has_edge(u, v) else 0\n",
    "\n",
    "    rows.append((u, v, cn, jaccard, genre_jaccard, genre_overlap_count, same_genre, co, label))\n",
    "\n",
    "# --- 3. Make DataFrame ---\n",
    "link_pred_df = pd.DataFrame(rows, columns=[\n",
    "    'source_id', 'target_id', 'common_neighbors', \n",
    "    'jaccard_coefficient', 'genre_jaccard', 'genre_overlap_count', 'same_genre',\n",
    "    'community_overlap', 'edge_exists'\n",
    "])\n",
    "\n",
    "# Normalize common neighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "link_pred_df['common_neighbors'] = scaler.fit_transform(link_pred_df[['common_neighbors']])\n",
    "\n",
    "print(link_pred_df.head())\n",
    "print(link_pred_df.shape)\n",
    "print(f\"\\nFeatures: {[col for col in link_pred_df.columns if col not in ['source_id', 'target_id', 'edge_exists']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4febb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating with CN_WEIGHT = 0.0\n",
      "Hits@10000: 0.00045367\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.1\n",
      "Hits@10000: 0.00045754\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.2\n",
      "Hits@10000: 0.00046054\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.3\n",
      "Hits@10000: 0.00046777\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.4\n",
      "Hits@10000: 0.00047358\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.5\n",
      "Hits@10000: 0.00047975\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.6\n",
      "Hits@10000: 0.00048645\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.7\n",
      "Hits@10000: 0.00049227\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.8\n",
      "Hits@10000: 0.00049667\n",
      "\n",
      "Evaluating with CN_WEIGHT = 0.9\n",
      "Hits@10000: 0.00050214\n",
      "\n",
      "Evaluating with CN_WEIGHT = 1.0\n",
      "Hits@10000: 0.00050425\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.0\n",
      "Hits@10000: 0.00044785\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.1\n",
      "Hits@10000: 0.00045032\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.2\n",
      "Hits@10000: 0.00045279\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.3\n",
      "Hits@10000: 0.00045825\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.4\n",
      "Hits@10000: 0.00046830\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.5\n",
      "Hits@10000: 0.00047975\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.6\n",
      "Hits@10000: 0.00049156\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.7\n",
      "Hits@10000: 0.00050073\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.8\n",
      "Hits@10000: 0.00050337\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 0.9\n",
      "Hits@10000: 0.00051077\n",
      "\n",
      "Evaluating with JACCARD_WEIGHT = 1.0\n",
      "Hits@10000: 0.00051342\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.0\n",
      "Hits@10000: 0.00053862\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.1\n",
      "Hits@10000: 0.00052575\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.2\n",
      "Hits@10000: 0.00050901\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.3\n",
      "Hits@10000: 0.00050108\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.4\n",
      "Hits@10000: 0.00049332\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.5\n",
      "Hits@10000: 0.00047975\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.6\n",
      "Hits@10000: 0.00047200\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.7\n",
      "Hits@10000: 0.00046107\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.8\n",
      "Hits@10000: 0.00045737\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 0.9\n",
      "Hits@10000: 0.00045331\n",
      "\n",
      "Evaluating with GENRE_JACCARD_WEIGHT = 1.0\n",
      "Hits@10000: 0.00045208\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.0\n",
      "Hits@10000: 0.00051324\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.1\n",
      "Hits@10000: 0.00050901\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.2\n",
      "Hits@10000: 0.00050266\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.3\n",
      "Hits@10000: 0.00049526\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.4\n",
      "Hits@10000: 0.00048909\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.5\n",
      "Hits@10000: 0.00047975\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.6\n",
      "Hits@10000: 0.00047587\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.7\n",
      "Hits@10000: 0.00046971\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.8\n",
      "Hits@10000: 0.00046354\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 0.9\n",
      "Hits@10000: 0.00045843\n",
      "\n",
      "Evaluating with GENRE_COUNT_WEIGHT = 1.0\n",
      "Hits@10000: 0.00045349\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.0\n",
      "Hits@10000: 0.00052470\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.1\n",
      "Hits@10000: 0.00053932\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.2\n",
      "Hits@10000: 0.00054496\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.3\n",
      "Hits@10000: 0.00052752\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.4\n",
      "Hits@10000: 0.00051130\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.5\n",
      "Hits@10000: 0.00047975\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.6\n",
      "Hits@10000: 0.00045349\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.7\n",
      "Hits@10000: 0.00044856\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.8\n",
      "Hits@10000: 0.00044697\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 0.9\n",
      "Hits@10000: 0.00044697\n",
      "\n",
      "Evaluating with SAME_GENRE_WEIGHT = 1.0\n",
      "Hits@10000: 0.00044697\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.0\n",
      "Hits@10000: 0.00044292\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.1\n",
      "Hits@10000: 0.00044626\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.2\n",
      "Hits@10000: 0.00044697\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.3\n",
      "Hits@10000: 0.00044856\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.4\n",
      "Hits@10000: 0.00045349\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.5\n",
      "Hits@10000: 0.00047975\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.6\n",
      "Hits@10000: 0.00051130\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.7\n",
      "Hits@10000: 0.00052752\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.8\n",
      "Hits@10000: 0.00054496\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 0.9\n",
      "Hits@10000: 0.00053932\n",
      "\n",
      "Evaluating with COMMUNITY_WEIGHT = 1.0\n",
      "Hits@10000: 0.00052470\n",
      "Optimal Weights:\n",
      "CN_WEIGHT: 1.0\n",
      "JACCARD_WEIGHT: 1.0\n",
      "GENRE_JACCARD_WEIGHT: 0.0\n",
      "GENRE_COUNT_WEIGHT: 0.0\n",
      "SAME_GENRE_WEIGHT: 0.2\n",
      "COMMUNITY_WEIGHT: 0.8\n"
     ]
    }
   ],
   "source": [
    "# ========== BACKTESTING THE FULL LINK PREDICTION MODEL ==========\n",
    "def predict(common_neighbors, jaccard_coefficient, genre_jaccard, genre_overlap_count, same_genre, community_overlap, \n",
    "            CN_WEIGHT, JACCARD_WEIGHT, GENRE_JACCARD_WEIGHT, GENRE_COUNT_WEIGHT, SAME_GENRE_WEIGHT, COMMUNITY_WEIGHT):\n",
    "    # Weighted sum model with all features\n",
    "    score = (CN_WEIGHT * common_neighbors +\n",
    "             JACCARD_WEIGHT * jaccard_coefficient +\n",
    "             GENRE_JACCARD_WEIGHT * genre_jaccard +\n",
    "             GENRE_COUNT_WEIGHT * genre_overlap_count +\n",
    "             SAME_GENRE_WEIGHT * same_genre +\n",
    "             COMMUNITY_WEIGHT * community_overlap)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Normalize genre_overlap_count for fair comparison\n",
    "scaler_genre = MinMaxScaler()\n",
    "link_pred_df['genre_overlap_count_norm'] = scaler_genre.fit_transform(link_pred_df[['genre_overlap_count']])\n",
    "\n",
    "# assume linear relationship (for ease of testing and efficiency of code)\n",
    "# iterate each weight from 0 to 1 with step size 0.1 holding fixed othes at 0.5\n",
    "\n",
    "optimal_weights = {}\n",
    "\n",
    "for iterating_val in ['CN_WEIGHT', 'JACCARD_WEIGHT', 'GENRE_JACCARD_WEIGHT', 'GENRE_COUNT_WEIGHT', 'SAME_GENRE_WEIGHT', 'COMMUNITY_WEIGHT']:\n",
    "\n",
    "    # store best hit value\n",
    "    best_hits = 0\n",
    "    optimal_weights[iterating_val] = 0\n",
    "\n",
    "    for weight in np.arange(0, 1.1, 0.1):\n",
    "        weights = {\n",
    "            'CN_WEIGHT': 0.5,\n",
    "            'JACCARD_WEIGHT': 0.5,\n",
    "            'GENRE_JACCARD_WEIGHT': 0.5,\n",
    "            'GENRE_COUNT_WEIGHT': 0.5,\n",
    "            'SAME_GENRE_WEIGHT': 0.5,\n",
    "            'COMMUNITY_WEIGHT': 0.5\n",
    "        }\n",
    "        weights[iterating_val] = weight\n",
    "\n",
    "        link_pred_df['predicted_score'] = link_pred_df.apply(\n",
    "            lambda row: predict(\n",
    "                row['common_neighbors'],\n",
    "                row['jaccard_coefficient'],\n",
    "                row['genre_jaccard'],\n",
    "                row['genre_overlap_count_norm'],\n",
    "                row['same_genre'],\n",
    "                row['community_overlap'],\n",
    "                weights['CN_WEIGHT'],\n",
    "                weights['JACCARD_WEIGHT'],\n",
    "                weights['GENRE_JACCARD_WEIGHT'],\n",
    "                weights['GENRE_COUNT_WEIGHT'],\n",
    "                weights['SAME_GENRE_WEIGHT'],\n",
    "                weights['COMMUNITY_WEIGHT']\n",
    "            ), axis=1\n",
    "        )\n",
    "\n",
    "        print(f\"\\nEvaluating with {iterating_val} = {weight:.1f}\")\n",
    "        \n",
    "        #calculate hits@k\n",
    "        K = 10000\n",
    "        top_k = link_pred_df.nlargest(K, 'predicted_score')\n",
    "        hits = top_k['edge_exists'].sum()\n",
    "        hits_at_k = hits / len(edges)\n",
    "        print(f\"Hits@{K}: {hits_at_k:.8f}\")\n",
    "\n",
    "        if hits_at_k > best_hits:\n",
    "            best_hits = hits_at_k\n",
    "            optimal_weights[iterating_val] = weight\n",
    "\n",
    "print(\"Optimal Weights:\")\n",
    "for key, value in optimal_weights.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# print(link_pred_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8df2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a evaluation function\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(df, graphMetric):\n",
    "    # ----- AUC -----\n",
    "    auc = roc_auc_score(df['label'], df[graphMetric])\n",
    "    print(f\"AUC for {graphMetric}: {auc}\")\n",
    "\n",
    "    # ----- Precision -----\n",
    "    y_pred = [1 if s >= 0.5 else 0 for s in df[graphMetric]]\n",
    "    precision = precision_score(df['label'], y_pred)\n",
    "    print(f\"Precision: {precision}\")    \n",
    "\n",
    "    # ----- Recall -----\n",
    "    recall = recall_score(df['label'], y_pred)\n",
    "    print(f\"Recall: {recall}\")  \n",
    "\n",
    "    # ----- Hits@K -----\n",
    "    K = 1000\n",
    "    edges = list(zip(df['u'], df['v']))\n",
    "    predicted_edges = pd.DataFrame({\n",
    "        'edge': edges,\n",
    "        'score': df[graphMetric]\n",
    "    })\n",
    "\n",
    "    # Sort by score descending\n",
    "    predicted_edges_sorted = predicted_edges.sort_values('score', ascending=False)\n",
    "\n",
    "    # Take top K edges\n",
    "    top_k_edges = set(predicted_edges_sorted['edge'].iloc[:K])\n",
    "\n",
    "    # Count how many true edges are in top K\n",
    "    hits = sum(1 for edge in edges if edge in top_k_edges)\n",
    "\n",
    "    # Compute Hits@K\n",
    "    hits_at_k = hits / len(edges)\n",
    "    print(f\"Hits@{K}: {hits_at_k:.4f}\")\n",
    "\n",
    "    # ----- Score Distribution -----\n",
    "    plt.hist(df[graphMetric], bins=50)\n",
    "    plt.title(\"Score Distribution\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2239d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9856104509915472\n",
      "Precision-Recall AUC: 0.4401496062625018\n",
      "Hits@100: 0.022985664854176965\n"
     ]
    }
   ],
   "source": [
    "# ========== AUC-ROC EVALUATION ==========\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(link_pred_df['edge_exists'], link_pred_df['predicted_score'])\n",
    "print(f\"AUC-ROC: {auc}\")\n",
    "\n",
    "# ========== Precision-Recall EVALUATION ==========\n",
    "from sklearn.metrics import precision_recall_curve, auc as auc_metric\n",
    "precision, recall, _ = precision_recall_curve(link_pred_df['edge_exists'], link_pred_df['predicted_score'])\n",
    "pr_auc = auc_metric(recall, precision)\n",
    "print(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "\n",
    "# ========== Hits@K EVALUATION ==========   \n",
    "def hits_at_k(df, k):\n",
    "    # Sort by predicted score descending\n",
    "    df_sorted = df.sort_values(by='predicted_score', ascending=False)\n",
    "    \n",
    "    # Get top K predictions\n",
    "    top_k = df_sorted.head(k)\n",
    "    \n",
    "    # Calculate Hits@K\n",
    "    hits = top_k['edge_exists'].sum()\n",
    "    total_positives = df['edge_exists'].sum()\n",
    "    \n",
    "    return hits / total_positives if total_positives > 0 else 0 \n",
    "hits_k = 100\n",
    "hits_at_100 = hits_at_k(link_pred_df, hits_k)\n",
    "print(f\"Hits@{hits_k}: {hits_at_100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a3b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final AUC-ROC: 0.986569192336466\n",
      "Final PR-AUC:  0.5158836687151468\n",
      "Precision @ 0.5: 0.6678571428571428\n",
      "Recall @ 0.5:    0.3235294117647059\n",
      "\n",
      "Hits@K Analysis:\n",
      "  Hits@10: 10 (Precision@10: 1.0000)\n",
      "  Hits@100: 100 (Precision@100: 1.0000)\n",
      "  Hits@1000: 798 (Precision@1000: 0.7980)\n",
      "  Hits@10000: 2857 (Precision@10000: 0.2857)\n",
      "  Hits@100000: 4031 (Precision@100000: 0.0403)\n"
     ]
    }
   ],
   "source": [
    "# ========== FINAL EVAL WITH OPTIMAL WEIGHTS ==========\n",
    "\n",
    "# create mapping of optimal weights\n",
    "optimal_weights = {\n",
    "    'CN_WEIGHT': 1.0,\n",
    "    'JACCARD_WEIGHT': 1.0,\n",
    "    'GENRE_JACCARD_WEIGHT': 0.0,\n",
    "    'GENRE_COUNT_WEIGHT': 0.0,\n",
    "    'SAME_GENRE_WEIGHT': 0.2,\n",
    "    'COMMUNITY_WEIGHT': 0.8\n",
    "}\n",
    "\n",
    "# run predict with optimal weights, store results in new column of dataframe\n",
    "link_pred_df['final_predicted_score'] = link_pred_df.apply(\n",
    "    lambda row: predict(\n",
    "        row['common_neighbors'],\n",
    "        row['jaccard_coefficient'],\n",
    "        row['genre_jaccard'],\n",
    "        row['genre_overlap_count_norm'],\n",
    "        row['same_genre'],\n",
    "        row['community_overlap'],\n",
    "        optimal_weights['CN_WEIGHT'],\n",
    "        optimal_weights['JACCARD_WEIGHT'],\n",
    "        optimal_weights['GENRE_JACCARD_WEIGHT'],\n",
    "        optimal_weights['GENRE_COUNT_WEIGHT'],\n",
    "        optimal_weights['SAME_GENRE_WEIGHT'],\n",
    "        optimal_weights['COMMUNITY_WEIGHT']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# evaulation\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, precision_recall_curve, auc as auc_metric\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== AUC-ROC EVALUATION ==========\n",
    "auc_roc = roc_auc_score(link_pred_df['edge_exists'], link_pred_df['final_predicted_score'])\n",
    "print(f\"Final AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# ========== Precision-Recall EVALUATION ==========\n",
    "precision, recall, _ = precision_recall_curve(link_pred_df['edge_exists'], link_pred_df['final_predicted_score'])\n",
    "pr_auc = auc_metric(recall, precision)\n",
    "print(f\"Final PR-AUC:  {pr_auc}\")\n",
    "\n",
    "# --- Precision & Recall at Threshold 0.5 (Normalized Score) ---\n",
    "# Normalize final score to [0,1] for thresholding\n",
    "min_score = link_pred_df['final_predicted_score'].min()\n",
    "max_score = link_pred_df['final_predicted_score'].max()\n",
    "link_pred_df['final_score_norm'] = (link_pred_df['final_predicted_score'] - min_score) / (max_score - min_score)\n",
    "\n",
    "threshold = 0.5\n",
    "y_pred = (link_pred_df['final_score_norm'] >= threshold).astype(int)\n",
    "prec_at_thresh = precision_score(link_pred_df['edge_exists'], y_pred)\n",
    "rec_at_thresh = recall_score(link_pred_df['edge_exists'], y_pred)\n",
    "\n",
    "print(f\"Precision @ {threshold}: {prec_at_thresh}\")\n",
    "print(f\"Recall @ {threshold}:    {rec_at_thresh}\")\n",
    "\n",
    "# ========== Hits@K EVALUATION ==========   \n",
    "def hits_at_k(df, k, score_col='final_predicted_score', label_col='edge_exists'):\n",
    "    # Sort by score descending\n",
    "    top_k = df.nlargest(k, score_col)\n",
    "    # Count true positives in top K\n",
    "    hits = top_k[label_col].sum()\n",
    "\n",
    "    return hits, hits / k\n",
    "K_values = [10, 100, 1000, 10000, 100000]\n",
    "print(\"\\nHits@K Analysis:\")\n",
    "for k in K_values:\n",
    "    hits, prec_at_k = hits_at_k(link_pred_df, k)\n",
    "    print(f\"  Hits@{k}: {hits} (Precision@{k}: {prec_at_k:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb803e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
