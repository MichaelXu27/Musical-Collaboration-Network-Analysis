{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1a8f6c",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline\n",
    "\n",
    "In this notebook, we will have the full data processing pipeline for the project.\n",
    "\n",
    "The goal is to have a clean collaboration network from the Discogs release XML files with:\n",
    "\n",
    "- Nodes = artists (Discogs artist IDs or their names)\n",
    "- Edges = collaborations between the artists\n",
    "- Edge attributes: This will be the release_year, which is actually what will help us in our train/test split processes\n",
    "\n",
    "The final folders with all our data for now will have the following:\n",
    "- inside data/final_data_raw: we will have the Discogs '*releases.xml' files which will go as our input in the processing steps\n",
    "- inside data/final_data_processed: we will have the processed Parquet files which will be the cleaned up data\n",
    "\n",
    "The outputs from the processing steps will be:\n",
    "- 'discogs_edges.parquet' with columns as: source_id, target_id, release_year\n",
    "- 'discogs_artists.parquet' with columns as: discogs_artist_id\n",
    "\n",
    "In the end, we will use the Spotify metadata, not really will but we might since it is mostly optional, but even if we do we will do it on a subset of artists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cf7e1",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febd7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Handling Imports\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Note: The following will be our directories, as described earlier\n",
    "RAW_DATA = '../data/final_data_raw'\n",
    "PROCESSED_DATA = '../data/final_data_processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78224b3f",
   "metadata": {},
   "source": [
    "## Parsing the XML file\n",
    "\n",
    "For each release in the XML:\n",
    "- We will extract the released field and pull out the 4-digit year\n",
    "- We will extract all artists under './artists/artist'\n",
    "    - We will also use id if present otherwise a fall back will be the name of the artist\n",
    "- We will only keep the releases with at least 2 artists such that there is some collaboration data\n",
    "- For such unordered paris of artists, u,v on the same release, we will create the source_id = min(u,v), target_id = max(u,v), and the release year\n",
    "\n",
    "Also each artists on each edge will be added to the nodes set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11e07e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/final_data_raw/discogs_20251101_releases.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m for_year = re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Note: Since yr will be in 4 digits\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Note: Since we have to parse a very large XML file, currently the raw dataset is 10GB+, we need to do it safely so as to not crash the memory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[43mET\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem.tag == \u001b[33m'\u001b[39m\u001b[33mrelease\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# Note: First step is to extract the release year\u001b[39;00m\n\u001b[32m     16\u001b[39m         release_year = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/xml/etree/ElementTree.py:1228\u001b[39m, in \u001b[36miterparse\u001b[39m\u001b[34m(source, events, parser)\u001b[39m\n\u001b[32m   1225\u001b[39m pullparser = XMLPullParser(events=events, _parser=parser)\n\u001b[32m   1227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1228\u001b[39m     source = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1229\u001b[39m     close_source = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/final_data_raw/discogs_20251101_releases.xml'"
     ]
    }
   ],
   "source": [
    "# Note: Now we have all the downloaded data in the XML file in the final_data_raw\n",
    "# Note: The path is ../data/final_data_raw/discogs_20251101_releases.xml\n",
    "# Note: To get the XML file\n",
    "xml_file = '../data/final_data_raw/discogs_20251101_releases.xml'\n",
    "# Note: First step is to initialize the containers\n",
    "edges, nodes = [], set()\n",
    "\n",
    "# Note: To get the release year from the xml file, we will use the following pattern\n",
    "for_year = re.compile(r'(\\d{4})') # Note: Since yr will be in 4 digits\n",
    "\n",
    "# Note: Since we have to parse a very large XML file, currently the raw dataset is 10GB+, we need to do it safely so as to not crash the memory\n",
    "for event, elem in ET.iterparse(xml_file, events=('end',)):\n",
    "    if elem.tag == 'release':\n",
    "\n",
    "        # Note: First step is to extract the release year\n",
    "        release_year = None\n",
    "        released_elem = elem.find('released')\n",
    "        if released_elem is not None and released_elem.text:\n",
    "            yr = for_year.search(released_elem.text)\n",
    "            if yr:\n",
    "                release_year = int(yr.group(1))\n",
    "        # Note: Next we get all the artists from the song\n",
    "        artists = [ar.findtext('id') or ar.findtext('name') for ar in elem.findall('./artists/artist')]\n",
    "        aritsts = [ar for ar in artists if ar]\n",
    "\n",
    "        # Note: After extracting the artists, we will create the nodes and the edges\n",
    "        if len(artists) >= 2:\n",
    "            for i in range(len(artists)):\n",
    "                nodes.add(aritsts[i])\n",
    "                for j in range(i+1, len(artists)):\n",
    "                    edges.append((artists[i], artists[j], release_year)) # Note: Release year is the edge attribute\n",
    "\n",
    "        # Note: This is the main step, doing elem.clear(), basicalyl frees up the memory for the processed part of XML\n",
    "        elem.clear()\n",
    "\n",
    "# Note: After all that is done, print len of nodes and edges to check what we have\n",
    "print(f'Nodes: {len(nodes)}, Edges: {len(edges)}')\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/7171140/using-python-iterparse-for-large-xml-files\n",
    "# Reference: https://lxml.de/parsing.html#incremental-event-parsing\n",
    "# Reference: https://boscoh.com/programming/reading-xml-serially.html\n",
    "# Reference: https://stackoverflow.com/questions/324214/what-is-the-fastest-way-to-parse-large-xml-docs-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccc045",
   "metadata": {},
   "source": [
    "## Convert to DataFrame\n",
    "\n",
    "Now, that we have our parsed XML file and we have teh edges with source_id, target_id, and release year and our nodes, we can convert them into a pd Dataframe. This will help us inspect the structure, run analysis and save teh data efficiently\n",
    "\n",
    "We should have:\n",
    "- Millions of edges\n",
    "- Hundreds of thousands to millions of artists\n",
    "\n",
    "Please note that this is not modifying the data but only structuring it for alter processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59cd032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_id target_id  release_year\n",
      "0        92     17757        1999.0\n",
      "1     33257      3482        2000.0\n",
      "2        96        95        1999.0\n",
      "3     12007    583687        1995.0\n",
      "4       645      5823        2000.0\n",
      "  discogs_artist_id\n",
      "0           9134149\n",
      "1            403139\n",
      "2           1983365\n",
      "3           4570205\n",
      "4           3566446\n",
      "7284440 Edges, 1045947 Nodes(Artists)\n"
     ]
    }
   ],
   "source": [
    "# Note: First convert the edges list to DataFrame\n",
    "edges_df = pd.DataFrame(edges, columns=['source_id', 'target_id', 'release_year'])\n",
    "# Note: Next we convert the nodes set to DF\n",
    "artists_df = pd.DataFrame({'discogs_artist_id': list(nodes)})\n",
    "print(edges_df.head())\n",
    "print(artists_df.head())\n",
    "print(f'{len(edges_df)} Edges, {len(artists_df)} Nodes(Artists)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f27f2",
   "metadata": {},
   "source": [
    "## Cleaning the Edges\n",
    "\n",
    "For the link prediction step, it is important to ensure that each edge has a valid release year, and so based on research a small percentage of Discgos entries might not include a year, so these must be removed\n",
    "\n",
    "- We will remove edges with release_year = None\n",
    "- This ensures the final dataset will be consisten for our time-based predictions\n",
    "\n",
    "(Realistically, we are not expecting a big decrease in the number of edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74782ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining edges: 5673764\n"
     ]
    }
   ],
   "source": [
    "# Note: As mentioned, removing edges with no release years\n",
    "edges_df = edges_df.dropna(subset=['release_year'])\n",
    "# Note: Later noticed that there are datasets with 0.0 as release_year, so just doing the following too\n",
    "edges_df = edges_df[edges_df['release_year'] != 0.0]\n",
    "edges_df['release_year'] = edges_df['release_year'].astype(int)\n",
    "\n",
    "print(f'Remaining edges: {len(edges_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093d55",
   "metadata": {},
   "source": [
    "## Saving the processed Parquet Files\n",
    "\n",
    "Now that we have data that is parsed and also cleaned, we will save it inside the data/final_data_processed folder\n",
    "\n",
    "We will save two files:\n",
    "- discogs_edges.parquet: Columns will be source_id, target_id, release_year\n",
    "- discogs_artists.parquet: Columns will be discogs_artist_id\n",
    "\n",
    "The reason for the Parquet format is it is highly compressed, fast to load, and very ideal for such large datasets with millions of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5520fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Defining our paths for output\n",
    "edges_output = os.path.join(PROCESSED_DATA, 'discogs_edges.parquet')\n",
    "artists_output = os.path.join(PROCESSED_DATA, 'discogs_artists.parquet')\n",
    "\n",
    "# Note: Saving our dataframes\n",
    "edges_df.to_parquet(edges_output, index=False)\n",
    "artists_df.to_parquet(artists_output, index = False)\n",
    "\n",
    "# Reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae011d4d",
   "metadata": {},
   "source": [
    "## Spotify Discussion\n",
    "\n",
    "Realistically speaking, we already have over 1 million nodes and over 5 million edges after cleaning, this dataset is already massive\n",
    "\n",
    "Link prediction works off of Edges + Graph Structure, for which we have all the rows we need (millions in fact). Spotify would genuinely just be 'extra decoration' to put it that way, it would not really be improving most of our graph link prediction methods, and so we will currently put it on hold and if everything works well with what we have then we do not really need it, since fetching the spotify data for all these artists on its own is damn near impossible. If it seems relevant we will run something for a top N frequent artists, if it seems like that will be handly later on. For now, this concludes our data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4a292",
   "metadata": {},
   "source": [
    "## General Link Predicition with Non-Graphical Measures\n",
    "\n",
    "Here, we will attempt to predict the same edges with a method that doesn't utilize graphical measures. If our hypothesis is correct, this should yield a worse outocme than above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8eec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  discogs_artist_id\n",
      "0           9134149\n",
      "1            403139\n",
      "2           1983365\n",
      "3           4570205\n",
      "4           3566446\n",
      "               id                    name         realname  \\\n",
      "0               1           The Persuader  Jesper Dahlbäck   \n",
      "1               2  Mr. James Barth & A.D.              NaN   \n",
      "2               3               Josh Wink   Josh Winkelman   \n",
      "3               4           Johannes Heil    Johannes Heil   \n",
      "4               5              Heiko Laux       Heiko Laux   \n",
      "...           ...                     ...              ...   \n",
      "9798342  16856335             Izumi Kohki              NaN   \n",
      "9798343  16856338                  Kate08        Kate Webb   \n",
      "9798344  16856341   The Evil B-Side Twins              NaN   \n",
      "9798345  16856347             Carol Lundy              NaN   \n",
      "9798346  16856350            문선 (Moonsun)              NaN   \n",
      "\n",
      "                                                   profile  \\\n",
      "0        Electronic artist working out of Stockholm, ac...   \n",
      "1                                                      NaN   \n",
      "2        Electronic music DJ, label owner, producer, an...   \n",
      "3        Electronic music producer, musician and live p...   \n",
      "4        German DJ and producer based in Berlin. He is ...   \n",
      "...                                                    ...   \n",
      "9798342                                                NaN   \n",
      "9798343                                                NaN   \n",
      "9798344                                                NaN   \n",
      "9798345                                                NaN   \n",
      "9798346                                                NaN   \n",
      "\n",
      "                data_quality  urls  namevariations  aliases  members  groups  \n",
      "0                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "1                    Correct   NaN             NaN      NaN      NaN     NaN  \n",
      "2                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "3                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "4                 Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "...                      ...   ...             ...      ...      ...     ...  \n",
      "9798342  Needs Major Changes   NaN             NaN      NaN      NaN     NaN  \n",
      "9798343           Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "9798344  Needs Major Changes   NaN             NaN      NaN      NaN     NaN  \n",
      "9798345  Needs Major Changes   NaN             NaN      NaN      NaN     NaN  \n",
      "9798346           Needs Vote   NaN             NaN      NaN      NaN     NaN  \n",
      "\n",
      "[9798347 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "df_artistsID = pd.read_parquet(\"../data/final_data_processed/discogs_artists.parquet\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# read the artists xml file so we can use the artist IDs from the parquet file to get artist names\n",
    "rows = []\n",
    "for event, elem in ET.iterparse(\"../data_raw/discogs_20251101_artists.xml\", events=(\"end\",)):\n",
    "    if elem.tag == \"artist\":   # repeating entry tag for Discogs artists\n",
    "        row = {child.tag: child.text for child in elem}\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        # free memory\n",
    "        elem.clear()\n",
    "        parent = elem.getparent() if hasattr(elem, \"getparent\") else None\n",
    "        if parent is not None:\n",
    "            while parent.getprevious() is not None:\n",
    "                del parent[0]\n",
    "\n",
    "df_artistsNames = pd.DataFrame(rows)\n",
    "print(df_artistsNames.head(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34150563",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "for name in df_artistsID['discogs_artist_id'][:1000]:\n",
    "    artist_name = df_artistsNames.loc[df_artistsNames['id'] == name, 'name'].values\n",
    "    if len(artist_name) > 0:\n",
    "        # print(artist_name[0])\n",
    "        names.append(artist_name[0])\n",
    "    else:\n",
    "        print(\"Name not found\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justin Bieber\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "load_dotenv()  # loads .env into environment\n",
    "\n",
    "client_id = os.getenv(\"SPOTIPY_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "auth_manager = SpotifyClientCredentials(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "# testing to see if the API call works\n",
    "artist = sp.artist(\"1uNFoZAHBGtllmzznpCI3s\")  # Justin Bieber ID\n",
    "print(artist[\"name\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ba6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method \n",
    "def get_artist(artist, market):\n",
    "    results = sp.search(q=f\"artist:{artist}\", type=\"artist\", limit=1, market=market)\n",
    "    items = results.get(\"artists\", {}).get(\"items\", [])\n",
    "\n",
    "    \n",
    "    return items[0] if items else None\n",
    "\n",
    "\n",
    "def get_artist_data(artist_name, market):\n",
    "    artist = get_artist(artist_name, market)\n",
    "    if artist:\n",
    "        return {\n",
    "                \"spotify_id\": artist[\"id\"],\n",
    "                \"spotify_name\": artist[\"name\"],\n",
    "                \"genres\": artist.get(\"genres\", []),\n",
    "                \"popularity\": artist.get(\"popularity\", None),\n",
    "                \"followers\": artist.get(\"followers\", {}).get(\"total\", None),\n",
    "            }\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
