{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1a8f6c",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline\n",
    "\n",
    "In this notebook, we will have the full data processing pipeline for the project.\n",
    "\n",
    "The goal is to have a clean collaboration network from the Discogs release XML files with:\n",
    "\n",
    "- Nodes = artists (Discogs artist IDs or their names)\n",
    "- Edges = collaborations between the artists\n",
    "- Edge attributes: This will be the release_year, which is actually what will help us in our train/test split processes\n",
    "\n",
    "The final folders with all our data for now will have the following:\n",
    "- inside data/final_data_raw: we will have the Discogs '*releases.xml' files which will go as our input in the processing steps\n",
    "- inside data/final_data_processed: we will have the processed Parquet files which will be the cleaned up data\n",
    "\n",
    "The outputs from the processing steps will be:\n",
    "- 'discogs_edges.parquet' with columns as: source_id, target_id, release_year\n",
    "- 'discogs_artists.parquet' with columns as: discogs_artist_id\n",
    "\n",
    "In the end, we will use the Spotify metadata, not really will but we might since it is mostly optional, but even if we do we will do it on a subset of artists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cf7e1",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febd7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Handling Imports\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Note: The following will be our directories, as described earlier\n",
    "RAW_DATA = '../data/final_data_raw'\n",
    "PROCESSED_DATA = '../data/final_data_processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78224b3f",
   "metadata": {},
   "source": [
    "## Parsing the XML file\n",
    "\n",
    "For each release in the XML:\n",
    "- We will extract the released field and pull out the 4-digit year\n",
    "- We will extract all artists under './artists/artist'\n",
    "    - We will also use id if present otherwise a fall back will be the name of the artist\n",
    "- We will only keep the releases with at least 2 artists such that there is some collaboration data\n",
    "- For such unordered paris of artists, u,v on the same release, we will create the source_id = min(u,v), target_id = max(u,v), and the release year\n",
    "\n",
    "Also each artists on each edge will be added to the nodes set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11e07e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 1045947, Edges: 7284440\n"
     ]
    }
   ],
   "source": [
    "# Note: Now we have all the downloaded data in the XML file in the final_data_raw\n",
    "# Note: The path is ../data/final_data_raw/discogs_20251101_releases.xml\n",
    "# Note: To get the XML file\n",
    "xml_file = '../data/final_data_raw/discogs_20251101_releases.xml'\n",
    "# Note: First step is to initialize the containers\n",
    "edges, nodes = [], set()\n",
    "\n",
    "# Note: To get the release year from the xml file, we will use the following pattern\n",
    "for_year = re.compile(r'(\\d{4})') # Note: Since yr will be in 4 digits\n",
    "\n",
    "# Note: Since we have to parse a very large XML file, currently the raw dataset is 10GB+, we need to do it safely so as to not crash the memory\n",
    "for event, elem in ET.iterparse(xml_file, events=('end',)):\n",
    "    if elem.tag == 'release':\n",
    "\n",
    "        # Note: First step is to extract the release year\n",
    "        release_year = None\n",
    "        released_elem = elem.find('released')\n",
    "        if released_elem is not None and released_elem.text:\n",
    "            yr = for_year.search(released_elem.text)\n",
    "            if yr:\n",
    "                release_year = int(yr.group(1))\n",
    "        # Note: Next we get all the artists from the song\n",
    "        artists = [ar.findtext('id') or ar.findtext('name') for ar in elem.findall('./artists/artist')]\n",
    "        aritsts = [ar for ar in artists if ar]\n",
    "\n",
    "        # Note: After extracting the artists, we will create the nodes and the edges\n",
    "        if len(artists) >= 2:\n",
    "            for i in range(len(artists)):\n",
    "                nodes.add(aritsts[i])\n",
    "                for j in range(i+1, len(artists)):\n",
    "                    edges.append((artists[i], artists[j], release_year)) # Note: Release year is the edge attribute\n",
    "\n",
    "        # Note: This is the main step, doing elem.clear(), basicalyl frees up the memory for the processed part of XML\n",
    "        elem.clear()\n",
    "\n",
    "# Note: After all that is done, print len of nodes and edges to check what we have\n",
    "print(f'Nodes: {len(nodes)}, Edges: {len(edges)}')\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/7171140/using-python-iterparse-for-large-xml-files\n",
    "# Reference: https://lxml.de/parsing.html#incremental-event-parsing\n",
    "# Reference: https://boscoh.com/programming/reading-xml-serially.html\n",
    "# Reference: https://stackoverflow.com/questions/324214/what-is-the-fastest-way-to-parse-large-xml-docs-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccc045",
   "metadata": {},
   "source": [
    "## Convert to DataFrame\n",
    "\n",
    "Now, that we have our parsed XML file and we have teh edges with source_id, target_id, and release year and our nodes, we can convert them into a pd Dataframe. This will help us inspect the structure, run analysis and save teh data efficiently\n",
    "\n",
    "We should have:\n",
    "- Millions of edges\n",
    "- Hundreds of thousands to millions of artists\n",
    "\n",
    "Please note that this is not modifying the data but only structuring it for alter processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59cd032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_id target_id  release_year\n",
      "0        92     17757        1999.0\n",
      "1     33257      3482        2000.0\n",
      "2        96        95        1999.0\n",
      "3     12007    583687        1995.0\n",
      "4       645      5823        2000.0\n",
      "  discogs_artist_id\n",
      "0           9134149\n",
      "1            403139\n",
      "2           1983365\n",
      "3           4570205\n",
      "4           3566446\n",
      "7284440 Edges, 1045947 Nodes(Artists)\n"
     ]
    }
   ],
   "source": [
    "# Note: First convert the edges list to DataFrame\n",
    "edges_df = pd.DataFrame(edges, columns=['source_id', 'target_id', 'release_year'])\n",
    "# Note: Next we convert the nodes set to DF\n",
    "artists_df = pd.DataFrame({'discogs_artist_id': list(nodes)})\n",
    "print(edges_df.head())\n",
    "print(artists_df.head())\n",
    "print(f'{len(edges_df)} Edges, {len(artists_df)} Nodes(Artists)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f27f2",
   "metadata": {},
   "source": [
    "## Cleaning the Edges\n",
    "\n",
    "For the link prediction step, it is important to ensure that each edge has a valid release year, and so based on research a small percentage of Discgos entries might not include a year, so these must be removed\n",
    "\n",
    "- We will remove edges with release_year = None\n",
    "- This ensures the final dataset will be consisten for our time-based predictions\n",
    "\n",
    "(Realistically, we are not expecting a big decrease in the number of edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74782ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining edges: 5673764\n"
     ]
    }
   ],
   "source": [
    "# Note: As mentioned, removing edges with no release years\n",
    "edges_df = edges_df.dropna(subset=['release_year'])\n",
    "# Note: Later noticed that there are datasets with 0.0 as release_year, so just doing the following too\n",
    "edges_df = edges_df[edges_df['release_year'] != 0.0]\n",
    "edges_df['release_year'] = edges_df['release_year'].astype(int)\n",
    "\n",
    "print(f'Remaining edges: {len(edges_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093d55",
   "metadata": {},
   "source": [
    "## Saving the processed Parquet Files\n",
    "\n",
    "Now that we have data that is parsed and also cleaned, we will save it inside the data/final_data_processed folder\n",
    "\n",
    "We will save two files:\n",
    "- discogs_edges.parquet: Columns will be source_id, target_id, release_year\n",
    "- discogs_artists.parquet: Columns will be discogs_artist_id\n",
    "\n",
    "The reason for the Parquet format is it is highly compressed, fast to load, and very ideal for such large datasets with millions of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5520fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Defining our paths for output\n",
    "edges_output = os.path.join(PROCESSED_DATA, 'discogs_edges.parquet')\n",
    "artists_output = os.path.join(PROCESSED_DATA, 'discogs_artists.parquet')\n",
    "\n",
    "# Note: Saving our dataframes\n",
    "edges_df.to_parquet(edges_output, index=False)\n",
    "artists_df.to_parquet(artists_output, index = False)\n",
    "\n",
    "# Reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae011d4d",
   "metadata": {},
   "source": [
    "## Spotify Discussion\n",
    "\n",
    "Realistically speaking, we already have over 1 million nodes and over 5 million edges after cleaning, this dataset is already massive\n",
    "\n",
    "Link prediction works off of Edges + Graph Structure, for which we have all the rows we need (millions in fact). Spotify would genuinely just be 'extra decoration' to put it that way, it would not really be improving most of our graph link prediction methods, and so we will currently put it on hold and if everything works well with what we have then we do not really need it, since fetching the spotify data for all these artists on its own is damn near impossible. If it seems relevant we will run something for a top N frequent artists, if it seems like that will be handly later on. For now, this concludes our data processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
